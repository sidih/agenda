<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8" /><!--THIS FILE IS GENERATED FROM AN XML MASTER. DO NOT EDIT (2)--><title>4. Topic modelling</title><meta http-equiv="x-ua-compatible" content="ie=edge" /><meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="description" content="" /><meta name="keywords" content="" /><meta name="author" content="Ajda Pretnar Žagar , Kristina Pahor de Maiti and Darja Fišer" /><meta name="generator" content="Text Encoding Initiative Consortium XSLT stylesheets" /><meta charset="utf-8" /><link href="https://www2.sistory.si/publikacije/themes/foundation/6/css/foundation.min.css" rel="stylesheet" type="text/css" /><link href="https://www2.sistory.si/publikacije/themes/css/foundation/6/sistory.css" rel="stylesheet" type="text/css" /><link href="https://cdnjs.cloudflare.com/ajax/libs/foundicons/3.0.0/foundation-icons.min.css" rel="stylesheet" type="text/css" /><link href="https://www2.sistory.si/publikacije/themes/plugin/TipueSearch/6.1/tipuesearch/css/normalize.css" rel="stylesheet" type="text/css" /><link href="https://www2.sistory.si/publikacije/themes/css/plugin/TipueSearch/6.1/my-tipuesearch.css" rel="stylesheet" type="text/css" /><link href="https://www2.sistory.si/publikacije/themes/plugin/ImageViewer/1.1.3/imageviewer.css" rel="stylesheet" type="text/css" /><style>
         .selflink:hover { opacity: 0.5;}
         .keywordlink:hover { opacity: 0.5;}
         .numberParagraphLink {text-decoration: none;}
         .numberParagraph:hover {color: black;}
      </style><script src="https://www2.sistory.si/publikacije/themes/foundation/6/js/vendor/jquery.js"></script><script src="https://www2.sistory.si/publikacije/themes/plugin/ImageViewer/1.1.3/imageviewer.js"></script></head><body id="TOP" itemscope="itemscope" itemtype="http://www.tei-c.org/ns/1.0/" itemprop="TEI"><script src="tipuesearch_content.js"></script><script src="https://www2.sistory.si/publikacije/themes/plugin/TipueSearch/6.1/tipuesearch/tipuesearch_set.js"></script><script src="https://www2.sistory.si/publikacije/themes/plugin/TipueSearch/6.1/tipuesearch/tipuesearch.min.js"></script><div class="column row"><header xmlns:html="http://www.w3.org/1999/xhtml"><div class="hide-for-large"><div id="header-bar"><div class="title-bar" data-responsive-toggle="publication-menu" data-hide-for="large"><button class="menu-icon" type="button" data-toggle=""></button><div class="title-bar-title">Menu</div><div class="title-bar-right"><a class="title-bar-title" href="https://sidih.si/20.500.12325/120"><i class="fi-home" style="color:white;"></i></a></div><div id="publication-menu" class="hide-for-large"><ul class="vertical menu" data-drilldown="" data-options="backButton: &lt;li class=&#34;js-drilldown-back&#34;&gt;&lt;a tabindex=&#34;0&#34;&gt;Back&lt;/a&gt;&lt;/li&gt;;"><li><a href="index.html">Cover Page</a></li><li><a href="cip.html">Colophon</a></li><li><a href="toc.html">TOC</a></li><li class="active"><a href="ch1.html">Chapters</a><ul class="vertical menu"><li><a href="ch1.html">1. Introduction</a></li><li><a href="ch2.html">2. Tutorial overview and instructions</a></li><li><a href="ch3.html">3. Parliamentary debates</a><ul class="vertical menu"><li><a href="ch3.html#ch3.1">3.1. Characteristics of parliamentary debates</a></li><li><a href="ch3.html#ch3.2">3.2. Parliamentary corpora</a></li><li><a href="ch3.html#ch3.3">3.3. The ParlaMint corpus</a></li></ul></li><li class="active"><a href="ch4.html">4. Topic modelling</a><ul class="vertical menu"><li><a href="ch4.html#ch4.1">4.1. The LDA method</a></li><li><a href="ch4.html#ch4.2">4.2. Characteristics of the LDA method</a></li><li><a href="ch4.html#ch4.3">4.3. Data preprocessing</a></li><li><a href="ch4.html#ch4.4">4.4. Limitations of LDA</a></li><li><a href="ch4.html#ch4.5">4.5. Topic modelling of parliamentary debates</a></li></ul></li><li><a href="ch5.html">5. Preparing for the analysis</a><ul class="vertical menu"><li><a href="ch5.html#ch5.1">5.1. Orange: setup and use</a></li><li><a href="ch5.html#ch5.2">5.2. Loading data into Orange</a></li><li><a href="ch5.html#ch5.3">5.3. Data overview</a></li><li><a href="ch5.html#ch5.4">5.4. Preparing and preprocessing the subcorpus</a><ul class="vertical menu"><li><a href="ch5.html#ch5.4.1">5.4.1. Removing unwanted speeches</a></li><li><a href="ch5.html#ch5.4.2">5.4.2. Removing unwanted words</a></li></ul></li></ul></li><li><a href="ch6.html">6. Analysis of parliamentary speeches</a><ul class="vertical menu"><li><a href="ch6.html#ch6.1">6.1. Topics of parliamentary speeches</a><ul class="vertical menu"><li><a href="ch6.html#ch6.1.1">6.1.1. Computing document vectors</a></li><li><a href="ch6.html#ch6.1.2">6.1.2. Topic modelling</a></li><li><a href="ch6.html#ch6.1.3">6.1.3. Topic definition</a></li><li><a href="ch6.html#ch6.1.4">6.1.4. Distribution of topics in a corpus</a></li></ul></li><li><a href="ch6.html#ch6.2">6.2. Topic map</a></li><li><a href="ch6.html#ch6.3">6.3. Topics before and during the pandemic</a></li></ul></li><li><a href="ch7.html">7. Conclusion</a></li></ul></li><li><a href="bibl.html">Bibliography</a></li><li><a href="acknw.html">Appendix</a></li><li><a href="summary.html">Summary</a></li><li><a href="#">English</a><ul class="menu"><li><a href="ch4-sl.html">Slovenščina</a></li></ul></li></ul></div></div></div></div><div class="show-for-large"><nav class="title-bar"><div class="title-bar-left"><a class="title-bar-title" href="https://sidih.si/20.500.12325/120"><i class="fi-home" style="color:white;"></i> <span>SI-DIH</span></a></div><div class="title-bar-right"><ul class="dropdown menu" data-dropdown-menu=""><li><a href="index.html">Cover Page</a></li><li><a href="cip.html">Colophon</a></li><li><a href="toc.html">TOC</a></li><li class="active"><a href="ch1.html">Chapters</a><ul class="menu"><li><a href="ch1.html">1. Introduction</a></li><li><a href="ch2.html">2. Tutorial overview and instructions</a></li><li><a href="ch3.html">3. Parliamentary debates</a><ul class="menu"><li><a href="ch3.html#ch3.1">3.1. Characteristics of parliamentary debates</a></li><li><a href="ch3.html#ch3.2">3.2. Parliamentary corpora</a></li><li><a href="ch3.html#ch3.3">3.3. The ParlaMint corpus</a></li></ul></li><li class="active"><a href="ch4.html">4. Topic modelling</a><ul class="menu"><li><a href="ch4.html#ch4.1">4.1. The LDA method</a></li><li><a href="ch4.html#ch4.2">4.2. Characteristics of the LDA method</a></li><li><a href="ch4.html#ch4.3">4.3. Data preprocessing</a></li><li><a href="ch4.html#ch4.4">4.4. Limitations of LDA</a></li><li><a href="ch4.html#ch4.5">4.5. Topic modelling of parliamentary debates</a></li></ul></li><li><a href="ch5.html">5. Preparing for the analysis</a><ul class="menu"><li><a href="ch5.html#ch5.1">5.1. Orange: setup and use</a></li><li><a href="ch5.html#ch5.2">5.2. Loading data into Orange</a></li><li><a href="ch5.html#ch5.3">5.3. Data overview</a></li><li><a href="ch5.html#ch5.4">5.4. Preparing and preprocessing the subcorpus</a><ul class="menu"><li><a href="ch5.html#ch5.4.1">5.4.1. Removing unwanted speeches</a></li><li><a href="ch5.html#ch5.4.2">5.4.2. Removing unwanted words</a></li></ul></li></ul></li><li><a href="ch6.html">6. Analysis of parliamentary speeches</a><ul class="menu"><li><a href="ch6.html#ch6.1">6.1. Topics of parliamentary speeches</a><ul class="menu"><li><a href="ch6.html#ch6.1.1">6.1.1. Computing document vectors</a></li><li><a href="ch6.html#ch6.1.2">6.1.2. Topic modelling</a></li><li><a href="ch6.html#ch6.1.3">6.1.3. Topic definition</a></li><li><a href="ch6.html#ch6.1.4">6.1.4. Distribution of topics in a corpus</a></li></ul></li><li><a href="ch6.html#ch6.2">6.2. Topic map</a></li><li><a href="ch6.html#ch6.3">6.3. Topics before and during the pandemic</a></li></ul></li><li><a href="ch7.html">7. Conclusion</a></li></ul></li><li><a href="bibl.html">Bibliography</a></li><li><a href="acknw.html">Appendix</a></li><li><a href="summary.html">Summary</a></li><li><a href="#">English</a><ul class="menu"><li><a href="ch4-sl.html">Slovenščina</a></li></ul></li></ul></div></nav></div><form action="search.html"><div class="row collapse"><div class="small-10 large-11 columns"><input type="text" name="q" id="tipue_search_input" placeholder="Your search text" /></div><div class="small-2 large-1 columns"><img type="button" class="tipue_search_button" /></div></div></form></header><section><div class="row"><div class="medium-2 columns show-for-medium"><p><a class="button" href="ch3.html" title="Previous: 3. Parliamentary debates">&lt;&lt;</a></p></div><div class="medium-8 small-12 columns"><h2 lang="en" class="maintitle"><span class="head" itemprop="head" id="head-17">4. Topic modelling</span></h2></div><div class="medium-2 columns show-for-medium text-right"><p><a class="button" href="ch5.html" title="Next: 5. Preparing for the analysis">&gt;&gt;</a></p></div></div><div class="row hide-for-medium"><div class="small-6 columns text-center"><p><a class="button" href="ch3.html" title="Previous: 3. Parliamentary debates">&lt;&lt;</a></p></div><div class="small-6 columns text-center"><p><a class="button" href="ch5.html" title="Next: 5. Preparing for the analysis">&gt;&gt;</a></p></div></div><div lang="en" class="chapter" id="ch4"><p id="p-26"><span><a class="numberParagraphLink" href="#p-26" title="number paragraph link"><span class="numberParagraph">1</span></a></span>To analyse parliamentary debates, we will use topic modelling, one of the text mining techniques used for researching large data sets. Given that several topic modelling methods exist, it is vital to know the advantages and disadvantages of each to choose the one that yields optimal results, which would provide quality results and ensure a critical interpretation of the results (<a class="link_ref" itemprop="ref" href="bibl.html#Shadrova.2021" title="Shadrova A. (2021). Topic models do not model topics Epistemological remarks and steps towards best practices. Journal of Data ...">Shadrova, 2021</a>). In this chapter, we present the selected method of topic modelling and some examples of its application to parliamentary discourse.</p><p id="p-27"><span><a class="numberParagraphLink" href="#p-27" title="number paragraph link"><span class="numberParagraph">2</span></a></span>Topic modelling is a popular technique for automatic text analysis, which extracts the main topics in a corpus. A topic model assigns each document in the corpus to one or more topics. These topics are not actual text topics but rather sets of words that co-occur with high probability and hence form a single topic. The researcher must then manually define or name the topic described with these sets of words.</p><p id="p-28"><span><a class="numberParagraphLink" href="#p-28" title="number paragraph link"><span class="numberParagraph">3</span></a></span>Various methods (algorithms) can perform topic modelling on a corpus (<a class="link_ref" itemprop="ref" href="bibl.html#Vayansky.2020" title="Vayansky I.  Kumar S. A. (2020). A review of topic modeling methods. Information Systems 94 101582.">Vayansky and Kumar, 2020</a>). One of the most frequent ones is LDA or <span style="font-style:italic" itemprop="hi">latent Dirichlet allocation</span>, which we will use in our analysis. The method was developed by <a class="link_ref" itemprop="ref" href="bibl.html#Pritchard.2000" title="Pritchard J. K. Stephens M.  Donnelly P. (2000). Inference of population structure using multilocus genotype data. Genetics 155...">Pritchard et al. (2000)</a> and adapted for text analysis by <a class="link_ref" itemprop="ref" href="bibl.html#Blei.2003" title="Blei D. M. Ng A. Y.  Jordan M. I. (2003). Latent dirichlet allocation. Journal of Machine Learning Research 3(Jan) 9931022.">Blei, Ng, and Jordan (2003)</a>. The method is best suited for processing large textual data sets which cannot be analysed manually due to their size.</p><div class="subchapter" id="ch4.1"><h3><span class="head" itemprop="head" id="head-18">4.1. The LDA method</span></h3><p id="p-29"><span><a class="numberParagraphLink" href="#p-29" title="number paragraph link"><span class="numberParagraph">1</span></a></span>The LDA method includes the following steps, performed in iteration:</p><ol itemprop="list" id="list-1"><li class="item" itemprop="item" id="d46e787">The algorithm first randomly allocates topics to the words in the corpus.</li></ol><div class="rules" itemprop="table" id="table-1"><table style="border-collapse:collapse;border-spacing:0;"><tr itemprop="row"><td class="center"><p style="text-align:center;" id="index-p-d46e795"><span class="numberParagraph">1</span><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">word</span></p> <p id="index-p-d46e801"><span class="numberParagraph">2</span><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">document</span></p></td><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">epidemic</span></td><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">crisis</span></td><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">tax</span></td><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">economy</span></td></tr><tr itemprop="row"><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">doc1</span></td><td class="left">topic 1</td><td class="left">topic 2</td><td class="left">topic 2</td><td class="left">topic 1</td></tr><tr itemprop="row"><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">doc2</span></td><td class="left">topic 1</td><td class="left">topic 1</td><td class="left">topic 2</td><td class="left">topic 1</td></tr><tr itemprop="row"><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">doc3</span></td><td class="left">topic 2</td><td class="left">topic 1</td><td class="left">topic 1</td><td class="left">topic 2</td></tr><tr itemprop="row"><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">doc4</span></td><td class="left">topic 1</td><td class="left">topic 2</td><td class="left">topic 2</td><td class="left">topic 2</td></tr></table></div><ol class="ordered" itemprop="list" start="2" id="list-2"><li class="item" itemprop="item" id="d46e884">Next, the algorithm counts the number of times a particular topic appears in each document (left table) and the number of times a certain topic is assigned to each word (right table).</li></ol><div class="rules" itemprop="table" id="table-2"><table style="border-collapse:collapse;border-spacing:0;"><tr itemprop="row"><td class="List_Paragraph"><div class="rules" itemprop="table"><table style="border-collapse:collapse;border-spacing:0;"><tr itemprop="row"><td class="left"></td><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">topic 1</span></td><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">topic 2</span></td></tr><tr itemprop="row"><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">doc1</span></td><td class="left">2</td><td class="left">2</td></tr><tr itemprop="row"><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">doc2</span></td><td class="left">3</td><td class="left">1</td></tr><tr itemprop="row"><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">doc3</span></td><td class="left">2</td><td class="left">2</td></tr><tr itemprop="row"><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">doc4</span></td><td class="left">1</td><td class="left">3</td></tr></table></div></td><td class="List_Paragraph"><div class="rules" itemprop="table"><table style="border-collapse:collapse;border-spacing:0;"><tr itemprop="row"><td class="left"></td><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">topic 1</span></td><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">topic 2</span></td></tr><tr itemprop="row"><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">epidemic</span></td><td class="left">3</td><td class="left">1</td></tr><tr itemprop="row"><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">crisis</span></td><td class="left">2</td><td class="left">2</td></tr><tr itemprop="row"><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">tax</span></td><td class="left">1</td><td class="left">3</td></tr><tr itemprop="row"><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">economy</span></td><td class="left">2</td><td class="left">2</td></tr></table></div></td></tr></table></div><ol class="ordered" itemprop="list" start="3" id="list-3"><li class="item" itemprop="item" id="d46e1000">The algorithm then assumes it no longer knows the topic of a given word.</li></ol><div class="rules" itemprop="table" id="table-5"><table style="border-collapse:collapse;border-spacing:0;"><tr itemprop="row"><td class="left"></td><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">epidemic</span></td><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">crisis</span></td><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">tax</span></td><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">economy</span></td></tr><tr itemprop="row"><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">doc1</span></td><td class="left"><span style="font-weight:bold;color:FF0000" itemprop="hi">?</span></td><td class="left">topic 2</td><td class="left">topic 2</td><td class="left">topic 1</td></tr><tr itemprop="row"><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">doc2</span></td><td class="left">topic 1</td><td class="left">topic 1</td><td class="left">topic 2</td><td class="left">topic 1</td></tr><tr itemprop="row"><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">doc3</span></td><td class="left">topic 2</td><td class="left">topic 1</td><td class="left">topic 1</td><td class="left">topic 2</td></tr><tr itemprop="row"><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">doc4</span></td><td class="left">topic 1</td><td class="left">topic 2</td><td class="left">topic 2</td><td class="left">topic 2</td></tr></table></div><ol class="ordered" itemprop="list" start="4" id="list-4"><li class="item" itemprop="item" id="d46e1084">Then, it updates both tables from step 2 by once again computing the frequency of topics in the corpus (left table) and the frequency of words in the topics (right table).</li></ol><div class="rules" itemprop="table" id="table-6"><table style="border-collapse:collapse;border-spacing:0;"><tr itemprop="row"><td class="left"></td><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">topic 1</span></td><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">topic 2</span></td></tr><tr itemprop="row"><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">doc1</span></td><td class="left"><span style="font-weight:bold;color:FF0000" itemprop="hi">1</span></td><td class="left">2</td></tr><tr itemprop="row"><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">doc2</span></td><td class="left">3</td><td class="left">1</td></tr><tr itemprop="row"><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">doc3</span></td><td class="left">2</td><td class="left">2</td></tr><tr itemprop="row"><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">doc4</span></td><td class="left">1</td><td class="left">3</td></tr></table></div><div class="rules" itemprop="table" id="table-7"><table style="border-collapse:collapse;border-spacing:0;"><tr itemprop="row"><td class="left"></td><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">topic 1</span></td><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">topic 2</span></td></tr><tr itemprop="row"><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">epidemic</span></td><td class="left"><span style="font-weight:bold;color:FF0000" itemprop="hi">2</span></td><td class="left">1</td></tr><tr itemprop="row"><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">crisis</span></td><td class="left">2</td><td class="left">2</td></tr><tr itemprop="row"><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">tax</span></td><td class="left">1</td><td class="left">3</td></tr><tr itemprop="row"><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">economy</span></td><td class="left">2</td><td class="left">2</td></tr></table></div><ol class="ordered" itemprop="list" start="5" id="list-5"><li class="item" itemprop="item" id="d46e1199">It computes the strength of the <span style="font-weight:bold" itemprop="hi">connection between a document and a topic</span> (the probability of the topic in a document: the blue rectangle) and the <span style="font-weight:bold" itemprop="hi">connection between the topic and a given word</span> (the probability of a word in a topic: the red rectangle).</li></ol><figure id="figure-1"><img class="imageviewer" src="https://sidih.si/iiif/2/entity|2001-3000|2177|Figure0.1_LDA-eng.png/full/max/0/default.jpg" data-high-res-src="https://sidih.si/cdn/2177/Figure0.1_LDA-eng.png" alt="" /><figcaption></figcaption></figure><br /><br /><ol class="ordered" itemprop="list" start="6" id="list-6"><li class="item" itemprop="item" id="d46e1210">The purple rectangle is a product of the red and the blue rectangle and represents the probability of a word in each topic. Based on the computed probability (purple rectangles), the method determines which topic will be assigned to a given document (green star which denotes a random allocation of the topic to the document, based on the computed word-topic probabilities). <br itemprop="lb" /> <figure id=""><img class="imageviewer" src="https://sidih.si/iiif/2/entity|2001-3000|2177|Figure0.2_LDA-prob-eng.png/full/max/0/default.jpg" data-high-res-src="https://sidih.si/cdn/2177/Figure0.2_LDA-prob-eng.png" alt="" /><figcaption></figcaption></figure><br /><br /> <br itemprop="lb" /> In short, the algorithm assigns a new topic (green star) based on the probability (purple rectangle). The probability distribution of topics in the document is based on the Dirichlet distribution, which postulates that the probability is never zero. Non-zero probability means that each word has at least a small chance of belonging to a less frequent topic and, concurrently, that even a lesser topic is present in a document. Once the topic is assigned to the word, the documents-words table is updated.</li></ol><div class="rules" itemprop="table" id="table-8"><table style="border-collapse:collapse;border-spacing:0;"><tr itemprop="row"><td class="left"></td><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">epidemic</span></td><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">crisis</span></td><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">tax</span></td><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">economy</span></td></tr><tr itemprop="row"><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">doc1</span></td><td class="left"><span style="font-weight:bold;color:FF0000" itemprop="hi">topic 2</span></td><td class="left">topic 2</td><td class="left">topic 2</td><td class="left">topic 1</td></tr><tr itemprop="row"><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">doc2</span></td><td class="left">topic 1</td><td class="left">topic 1</td><td class="left">topic 2</td><td class="left">topic 1</td></tr><tr itemprop="row"><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">doc3</span></td><td class="left">topic 2</td><td class="left">topic 1</td><td class="left">topic 1</td><td class="left">topic 2</td></tr><tr itemprop="row"><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">doc4</span></td><td class="left">topic 1</td><td class="left">topic 2</td><td class="left">topic 2</td><td class="left">topic 2</td></tr></table></div><ol class="ordered" itemprop="list" start="7" id="list-7"><li class="item" itemprop="item" id="d46e1302">Based on the new value from the table in step 6, the algorithm updates both tables: the topic-document and the word-topic table.</li></ol><div class="rules" itemprop="table" id="table-9"><table style="border-collapse:collapse;border-spacing:0;"><tr itemprop="row"><td class="left"></td><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">topic 1</span></td><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">topic 2</span></td></tr><tr itemprop="row"><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">doc1</span></td><td class="left">1</td><td class="left"><span style="font-weight:bold;color:FF0000" itemprop="hi">3</span></td></tr><tr itemprop="row"><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">doc2</span></td><td class="left">3</td><td class="left">1</td></tr><tr itemprop="row"><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">doc3</span></td><td class="left">2</td><td class="left">2</td></tr><tr itemprop="row"><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">doc4</span></td><td class="left">1</td><td class="left">3</td></tr></table></div><div class="rules" itemprop="table" id="table-10"><table style="border-collapse:collapse;border-spacing:0;"><tr itemprop="row"><td class="left"></td><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">topic 1</span></td><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">topic 2</span></td></tr><tr itemprop="row"><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">epidemic</span></td><td class="left">2</td><td class="left"><span style="font-weight:bold;color:FF0000" itemprop="hi">2</span></td></tr><tr itemprop="row"><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">crisis</span></td><td class="left">2</td><td class="left">2</td></tr><tr itemprop="row"><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">tax</span></td><td class="left">1</td><td class="left">3</td></tr><tr itemprop="row"><td class="left"><span style="font-weight:bold;font-variant: small-caps" itemprop="hi">economy</span></td><td class="left">2</td><td class="left">2</td></tr></table></div><ol class="ordered" itemprop="list" start="8" id="list-8"><li class="item" itemprop="item" id="d46e1416">The procedure is repeated until the topic assignments stop changing. The result is a topic model. The topic is defined by a set of words which frequently co-occur in the text.</li></ol><p id="p-32"><span><a class="numberParagraphLink" href="#p-32" title="number paragraph link"><span class="numberParagraph">2</span></a></span>Above, we have described the Gibbs sampling version of LDA. Please note that Orange uses variational inference instead of Gibbs sampling. Gibbs sampling is much more precise, while variational inference is faster for larger data sets.</p></div><div class="subchapter" id="ch4.2"><h3><span class="head" itemprop="head" id="head-19">4.2. Characteristics of the LDA method</span></h3><p id="p-33"><span><a class="numberParagraphLink" href="#p-33" title="number paragraph link"><span class="numberParagraph">1</span></a></span>LDA is based on multiple assumptions. The first one is that the topic is defined by the words that frequently appear together. LDA is a language-independent method since it merges words into groups based on their occurrence in the text and not on their meaning. The same method can thus be used on corpora in different languages. At the same time, each word can be assigned to multiple topics; however, its probability in each topic will vary.</p><p id="p-34"><span><a class="numberParagraphLink" href="#p-34" title="number paragraph link"><span class="numberParagraph">2</span></a></span>The second assumption is that not all topics in the corpus appear equally often, and that they are unrelated. LDA does not define potential connections between the topics; however, it shows the probability distribution of topics in the text, which defines their importance in each document. Each document contains several topics, with one topic usually standing out (i.e., the document contains more words associated with the main topic compared to other topics).</p><p id="p-35"><span><a class="numberParagraphLink" href="#p-35" title="number paragraph link"><span class="numberParagraph">3</span></a></span>The third assumption is that the number of topics is predefined. The researcher must first set the number of topics into which the algorithm sorts the documents. The optimal number of topics for a given corpus is the one at which it is easiest to interpret viable topics for given sets of words that must be informative in terms of the research problem. Researchers, therefore, typically apply the topic modelling procedure several times for a given research problem, each time setting a different number of topics, and assessing the informativeness of the word sets that the algorithm extracted from the corpus. Some researchers also use additional statistical tests to adjudicate between results of models with different numbers of topics (<a class="link_ref" itemprop="ref" href="bibl.html#Smith.2019" title="Smith N.  Graham T. (2019). Mapping the antivaccination movement on Facebook. Information Communication  Society 22(9) 13101327...">Smith and Graham, 2019</a>). Even though a suitable number of topics differs from case to case, the usual number ranges between 5 and 50 topics (<a class="link_ref" itemprop="ref" href="bibl.html#Arun.2010" title="Arun R. Suresh V. Veni Madhavan C.  Murthy N. (2010). On finding the natural number of topics with latent dirichlet allocation ...">Arun et al., 2010</a>) and many papers go with 20 topics (<a class="link_ref" itemprop="ref" href="bibl.html#Zhao.2015" title="Zhao W. Chen J. J. Perkins R. Liu Z. Ge W. Ding Y.  Zou W. (2015). A heuristic approach to determine an appropriate number of t...">Zhao et al., 2015</a>; <a class="link_ref" itemprop="ref" href="bibl.html#Gkoumas.2018" title="Gkoumas D. Pontiki M. Papanikolaou K.  Papageorgiou H. (2018). Exploring the Political Agenda of the Greek Parliament Plenary S...">Gkoumas et al., 2018</a>; <a class="link_ref" itemprop="ref" href="bibl.html#Rosa.2021" title="Rosa A. B. Gudowsky N.  Repo P. (2021). Sensemaking and lensshaping Identifying citizen contributions to foresight through comp...">Rosa et al., 2021</a>).</p><p id="p-36"><span><a class="numberParagraphLink" href="#p-36" title="number paragraph link"><span class="numberParagraph">4</span></a></span>The fourth assumption is that word order in a corpus is not important. LDA works based on the so-called bag of words which does not consider the linguistic structure or specific connections between the words. This assumption is problematic because the word order is one of the key characteristics of language.</p><p id="p-37"><span><a class="numberParagraphLink" href="#p-37" title="number paragraph link"><span class="numberParagraph">5</span></a></span>The results are importantly affected by document order, as the method randomly assigns topics to documents at the beginning. If the document order changes, the initial assignments will also change. The temporal sequence (i.e., the timestamp) of the documents can also importantly affect the characteristics of the documents (<a class="link_ref" itemprop="ref" href="bibl.html#Vayansky.2020" title="Vayansky I.  Kumar S. A. (2020). A review of topic modeling methods. Information Systems 94 101582.">Vayansky and Kumar, 2020</a>).</p></div><div class="subchapter" id="ch4.3"><h3><span class="head" itemprop="head" id="head-20">4.3. Data preprocessing</span></h3><p id="p-38"><span><a class="numberParagraphLink" href="#p-38" title="number paragraph link"><span class="numberParagraph">1</span></a></span>Before topic modelling, the data requires preparation which usually includes tokenisation (splitting the text into tokens, usually words, numbers, and punctuation), lemmatisation (assigning the base form to each token), and part-of-speech tagging (assigning the part of speech, e.g., verb, to each token). The procedure enables us to perform topic modelling on a single word form. Research has shown that lemmatisation and limiting the tokens to nouns improve the algorithm's speed and results. The improvement shows in the coherence or the sensible relations between word sets, based on which it is easier to assign a topic (<a class="link_ref" itemprop="ref" href="bibl.html#Martin.2015" title="Martin F.  Johnson M. (2015). More efficient topic modelling through a noun only approach. 111115.">Martin and Johnson, 2015</a>). Differentiating by part-of-speech tags can also be used for answering different research questions. <a class="link_ref" itemprop="ref" href="bibl.html#Zwaan.2016" title="van der Zwaan J. M. Marx M.  Kamps J. (2016). Validating CrossPerspective Topic Modeling for Extracting Political Parties Posit...">Van der Zwaan et al. (2016)</a> performed topic modelling on nouns to retrieve topics. They then ran the algorithm on verbs, adjectives, and adverbs and used the results to elicit the positions of the MPs. Before using LDA, we usually remove overly common words from the corpus: words that can be too general (e.g., pronouns, prepositions) or too specific for the genre (e.g., words of address, such as <span style="font-style:italic" itemprop="hi">esteemed</span> in the parliamentary corpus). Depending on the research problem, very rare words, punctuation, capital letters, etc. can also be removed (<a class="link_ref" itemprop="ref" href="bibl.html#Smith.2019" title="Smith N.  Graham T. (2019). Mapping the antivaccination movement on Facebook. Information Communication  Society 22(9) 13101327...">Smith and Graham, 2019</a>)</p></div><div class="subchapter" id="ch4.4"><h3><span class="head" itemprop="head" id="head-21">4.4. Limitations of LDA</span></h3><p id="p-39"><span><a class="numberParagraphLink" href="#p-39" title="number paragraph link"><span class="numberParagraph">1</span></a></span>One of the limitations of LDA is that the method requires long texts for good results as it is based on word distributions, which are spurious in shorter texts. LDA is thus not appropriate for topic modelling of tweets, user reviews, or poetry. Even though it could be used to analyse, for example, Facebook posts (see <a class="link_ref" itemprop="ref" href="bibl.html#Serrano.2019" title="Serrano J. C. M. Shahrezaye M. Papakyriakopoulos O.  Hegelich S. (2019). The rise of Germanys AfD A social media analysis. 2142...">Serrano et al., 2019</a>), it is recommended to use other topic modelling methods for such tasks (<a class="link_ref" itemprop="ref" href="bibl.html#Albalawi.2020" title="Albalawi R. Yeap T. H.  Benyoucef M. (2020). Using topic modeling methods for shorttext data A comparative analysis. Frontiers ...">Albalawi et al., 2020</a>; <a class="link_ref" itemprop="ref" href="bibl.html#Morstatter.2018" title="Morstatter F. Shao Y. Galstyan A.  Karunasekera S. (2018). From altright to altrechts Twitter analysis of the 2017 German feder...">Morstatter et al., 2018</a>). Parliamentary speeches are usually long enough to achieve good results with LDA. However, certain speeches can be very short, and it is wise to remove them before running the analysis (<a class="link_ref" itemprop="ref" href="ch5.html#ch5.4" title="5.4. Preparing and preprocessing the subcorpus">Chapter 5.4</a>).</p><p id="p-40"><span><a class="numberParagraphLink" href="#p-40" title="number paragraph link"><span class="numberParagraph">2</span></a></span>Another limitation is the assumption that words, just like the documents and the topics, are not co-dependent, which is linguistically imprecise on the one hand and does not allow for an analysis of correlations between words or between documents on the other. LDA suffices since these correlations are not the focal point for many research problems. However, if correlation is important (e.g., if we are interested in topic progression over time), there are more suitable methods, such as dynamic topic modelling (<a class="link_ref" itemprop="ref" href="bibl.html#M%C3%BCller-Hansen.2021" title="MüllerHansen F. Callaghan M. W. Lee Y. T. Leipprand A. Flachsland C.  Minx J. C. (2021). Who cares about coal Analyzing 70 year...">Müller-Hansen et al., 2021</a>).</p><p id="p-41"><span><a class="numberParagraphLink" href="#p-41" title="number paragraph link"><span class="numberParagraph">3</span></a></span>LDA will also underperform if the text does not address the topic coherently but touches upon the topic with a few words only. On the other hand, the method works extremely well for longer, thematically well-defined texts, such as news, academic papers, political speeches, and certain literary genres.</p><p id="p-42"><span><a class="numberParagraphLink" href="#p-42" title="number paragraph link"><span class="numberParagraph">4</span></a></span>The next limitation is related to the number of topics the researcher has to define autonomously and is usually the result of trial and error. <a class="link_ref" itemprop="ref" href="bibl.html#Allen.2020" title="Allen C.  Murdock J. (2020). LDA topic modeling Contexts for the history  philosophy of science.">Allen and Murdock (2020)</a> warn about overly specific topics representing only small sections of the text when the number of requested topics is high, making it difficult to establish thematic relations between texts. Conversely, when the number of topics is very limited, they will frequently be too general and thus uninformative to the research.</p><p id="p-43"><span><a class="numberParagraphLink" href="#p-43" title="number paragraph link"><span class="numberParagraph">5</span></a></span>As a final limitation, we can mention the difficulty of interpreting topic modelling results, including how the results are published. As the result of topic modelling are individual sets of words, there is a danger that the researcher will recognize a pattern in them even when none is present, meaning they will identify the topics they had expected (<a class="link_ref" itemprop="ref" href="bibl.html#Shadrova.2021" title="Shadrova A. (2021). Topic models do not model topics Epistemological remarks and steps towards best practices. Journal of Data ...">Shadrova, 2021</a>). It is thus vital to consider the number of inspected words when defining a topic. The results can differ if the researcher assigns topics based on the first ten or thirty words provided by the algorithm (<a class="link_ref" itemprop="ref" href="bibl.html#Allen.2020" title="Allen C.  Murdock J. (2020). LDA topic modeling Contexts for the history  philosophy of science.">Allen and Murdock, 2020</a>). Qualitative reading and understanding the original text segments in which the top listed words appear are crucial for accurately interpreting word sets and identifying topics. When working in a group, defining the common guidelines for topic identification in advance is also recommended.</p><p id="p-44"><span><a class="numberParagraphLink" href="#p-44" title="number paragraph link"><span class="numberParagraph">6</span></a></span>Topic modelling with the LDA method is thus not a one-size-fits-all solution that could provide the researcher with robust conclusions without a critical analysis of the results. Understanding the limitations of different topic modelling methods is key to successfully using them for research purposes, since quantitative, automated methods can successfully augment the researcher's analytical abilities, but they cannot replace human interpretation (<a class="link_ref" itemprop="ref" href="bibl.html#Grimmer.2013" title="Grimmer J.  Stewart B. M. (2013). Text as data The promise and pitfalls of automatic content analysis methods for political tex...">Grimmer and Stewart, 2013</a>). Nonetheless, the topic modelling technique provides an important advantage over the manual approach, specifically with regard to the processing of large data sets, enabling a more robust data-based generalisation than using a small-sample analysis (<a class="link_ref" itemprop="ref" href="bibl.html#Jacobs.2019" title="Jacobs T.  Tschötschel R. (2019). Topic models meet discourse analysis A quantitative tool for a qualitative approach. Internat...">Jacobs and Tschötschel, 2019</a>). Moreover, topic modelling enables greater objectivity of the results (<a class="link_ref" itemprop="ref" href="bibl.html#M%C3%BCller-Hansen.2021" title="MüllerHansen F. Callaghan M. W. Lee Y. T. Leipprand A. Flachsland C.  Minx J. C. (2021). Who cares about coal Analyzing 70 year...">Müller-Hansen et al., 2021</a>), even though the technique is not entirely objective due to the aforementioned topic definition process based on word sets.</p><p id="p-45"><span><a class="numberParagraphLink" href="#p-45" title="number paragraph link"><span class="numberParagraph">7</span></a></span>On the other hand, this is one of the advantages of the technique as the algorithm does not give direct answers but forces the researcher to consider the context when forming the final results (<a class="link_ref" itemprop="ref" href="bibl.html#Schmidt.2012" title="Schmidt B. M. (2012). Words alone Dismantling topic models in the humanities. Journal of Digital Humanities 2(1) 4965.">Schmidt, 2012</a>). Topic modelling also enhances systematisation of the analysis and enables a comparatively better reproducibility of the results (<a class="link_ref" itemprop="ref" href="bibl.html#Jacobs.2019" title="Jacobs T.  Tschötschel R. (2019). Topic models meet discourse analysis A quantitative tool for a qualitative approach. Internat...">Jacobs and Tschötschel, 2019</a>). The popularity and relevance of the technique for the research in the humanities and social sciences are evident from the many publications that use topic modelling as a part of their methodology (see <a class="link_ref" itemprop="ref" href="ch4.html#ch4.5" title="4.5. Topic modelling of parliamentary debates">Chapter 4.5</a>).</p></div><div class="subchapter" id="ch4.5"><h3><span class="head" itemprop="head" id="head-22">4.5. Topic modelling of parliamentary debates</span></h3><p id="p-46"><span><a class="numberParagraphLink" href="#p-46" title="number paragraph link"><span class="numberParagraph">1</span></a></span>In the humanities and social sciences, particularly in political science, topic modelling is increasingly used as an important technique to complement established, qualitative analytical approaches in analysing large data sets. The results of topic modelling may inform the qualitative analysis (e.g., the researcher can identify relevant texts about a specific topic) or can be used as the principal outcome of the analysis. In this chapter, we provide some examples of both from recent applications of the topic modelling technique to parliamentary data.</p><p id="p-47"><span><a class="numberParagraphLink" href="#p-47" title="number paragraph link"><span class="numberParagraph">2</span></a></span>Topic modelling allows us to <span style="font-weight:bold" itemprop="hi">identify the topics </span>addressed in parliamentary speeches. <a class="link_ref" itemprop="ref" href="bibl.html#Schuler.2020" title="Schuler P. (2020). Position taking or position ducking A theory of public debate in singleparty legislatures. Comparative Polit...">Schuler (2020)</a>, for example, used LDA to analyse the debates in the Vietnamese parliament and compared the results with topics from the news, also extracted with LDA, and with the list of areas under the direct management of the Communist Party (CP). He analysed whether MPs in an authoritarian system, such as the Vietnamese, express their opinions and actively debate important topics. He discovered that they debate only topics outside the CP's direct management, with the party encouraging such debates to pressure the government and blame it for the outcomes of the policies. However, the party does not encourage debates pertaining to areas directly managed by the CP committees. Moreover, debates concerning topics open for discussion do not involve all MPs but mostly those who are not members of the CP and were elected as full-time representatives.</p><p id="p-48"><span><a class="numberParagraphLink" href="#p-48" title="number paragraph link"><span class="numberParagraph">3</span></a></span><a class="link_ref" itemprop="ref" href="bibl.html#Chizhik.2021" title="Chizhik A. V.  Sergeyev D. A. (2021). Exploring the Parliamentary Discourse of the Russian Federation Using Topic Modeling Appr...">Chizhik and Sergeyev (2021)</a> also used topic modelling to discover topics in parliamentary debates by analysing three decades of Russian MPs' speeches. They researched whether the activity of the parliamentary parties is related to the public's scepticism regarding the multiparty system as a basis for democracy. They established that parties in the Russian parliament focus mostly on foreign affairs, the economy, and the balance of power between different branches of the government. At the same time, other social issues generate much less debate. Furthermore, speeches from all parties, but especially from the long-established ones, show a strong prevalence for ideological and propagandistic discourse.</p><p id="p-49"><span><a class="numberParagraphLink" href="#p-49" title="number paragraph link"><span class="numberParagraph">4</span></a></span>We saw how topic modelling enables acquiring a general overview of the material, which can suffice if the researcher’s aim is, for example, to observe the frequency of topics under consideration in the parliament. But topic modelling can also be used to retrieve more specific results. As parliamentary corpora are usually rich with metadata, <span style="font-weight:bold" itemprop="hi">topics can be explored in relation to other variables </span>(such as gender, age, party affiliation, mandate etc.), which elicits the topics that stand out most when the selected variable changes. We can thus observe how popular a topic was through time or with a certain party. <a class="link_ref" itemprop="ref" href="bibl.html#Curran.2018" title="Curran B. Higham K. Ortiz E.  Vasques Filho D. (2018). Look whos talking Twomode networks as representations of a topic model o...">Curran et al. (2018)</a> used LDA and the analysis of complex networks to elicit topics in the New Zealand parliament, which they then related to MPs and the parties. In this way, they not only retrieved popular topics of parliamentary debates for different periods and interpreted them in the context of external events (e.g., the 2011 earthquake) but also defined the interest of a party in each topic. Their results showed that the Labour Party debated the real estate crisis much more ardently than the then-governing National Party. The latter claimed most of the debate, while the contribution of other parties decreased over time. The MPs' specialisations for different topics also decreased, which is evident from the large number of topics addressed by the majority of the MPs.</p><p id="p-50"><span><a class="numberParagraphLink" href="#p-50" title="number paragraph link"><span class="numberParagraph">5</span></a></span><a class="link_ref" itemprop="ref" href="bibl.html#deCampos.2021" title="de Campos L. M. FernandezLuna J. M. Huete J. F.  RedondoExpósito L. (2021). LDAbased term profiles for expert finding in a poli...">De Campos et al. (2021)</a> used LDA in combination with the available metadata to create thematic profiles of Spanish MPs which reflect the subject matters they discuss in the parliament. Metadata was also used in research by <a class="link_ref" itemprop="ref" href="bibl.html#H%C3%B8yland.2019" title="Høyland B.  Søyland M. G. (2019). Electoral reform and parliamentary debates. Legislative Studies Quarterly 44(4) 593615.">Høyland and Søyland (2019)</a>. In 1919, Norway changed its electoral system to become substantially more dependent on party politics, reducing MPs’ autonomy. Høyland and Søyland investigated whether the change in the political system affected the topics in parliamentary debates. They used a version of LDA called structural topic modelling (STM), which considers both the word distributions and the selected metadata when computing the results. They determined that the topic distribution clearly shows that institutional organisation influences the behaviour of the MPs. After the reform that emphasised party politics, the topics showing clear ideological differences between the parties became more frequent, while MPs gave fewer speeches that directly criticized other MPs. Furthermore, MPs more frequently discussed topics of general interest (e.g., the educational system) and more rarely topics related directly to the issues of their constituents (e.g., improving the infrastructure of a remote town).</p><p id="p-51"><span><a class="numberParagraphLink" href="#p-51" title="number paragraph link"><span class="numberParagraph">6</span></a></span>Topic modelling enables <span style="font-weight:bold" itemprop="hi">exploring the context and selecting topics related to a given concept</span>. <a class="link_ref" itemprop="ref" href="bibl.html#M%C3%BCller-Hansen.2021" title="MüllerHansen F. Callaghan M. W. Lee Y. T. Leipprand A. Flachsland C.  Minx J. C. (2021). Who cares about coal Analyzing 70 year...">Müller-Hansen et al. (2021)</a> used a version of LDA called dynamic topic modelling (DTM), which enables the analysis of topics through time.<span id="ftn3_return"><a class="notelink" title="Time-based topic analysis can be done with plain LDA, but a separate topic model must be built for each period; the topics must be interpreted and the…" href="#ftn3"><sup>3</sup></a></span> They analysed seventy years of German parliamentary debates on coal and explored how they changed through time. The debates from the early years of the corpus show that the MPs considered coal the driver of economic progress and the guarantee of energy safety. Conversely, in recent years MPs primarily talked about energy transition, a general departure from coal, and the flourishing of renewable energy sources. Furthermore, the researchers also established that smaller and younger parties (e.g., the Greens) talk about coal in the context of energy transition and environmental protection more frequently than the other parties.</p><p id="p-52"><span><a class="numberParagraphLink" href="#p-52" title="number paragraph link"><span class="numberParagraph">7</span></a></span>Topic modelling can <span style="font-weight:bold" itemprop="hi">explore the context of a topic or the interplay of topics</span>. <a class="link_ref" itemprop="ref" href="bibl.html#Bl%C3%A4tte.2020" title="Blätte A. Gehlhar S.  Leonhardt C. (2020). The Europeanization of Parliamentary Debates on Migration in Austria France Germany ...">Blätte et al. (2020)</a> aimed to discover how frequently migration is addressed in common European politics. They used LDA to create a topic model of parliamentary debates from Austria, France, Germany, and the Netherlands. Then, they selected the three topics which best represented migration and European matters and retrieved all speeches with a high frequency of the two issues. The results show that the debate on migration was predominantly an internal issue in the larger two countries investigated (France and Germany). Particularly in Germany, the European aspect practically disappeared, while the smaller two countries (Austria and the Netherlands) had a larger share of speeches discussing migration from the European perspective.</p><p id="p-53"><span><a class="numberParagraphLink" href="#p-53" title="number paragraph link"><span class="numberParagraph">8</span></a></span>In this tutorial, we partially rely on the methodology used by <a class="link_ref" itemprop="ref" href="bibl.html#Curran.2018" title="Curran B. Higham K. Ortiz E.  Vasques Filho D. (2018). Look whos talking Twomode networks as representations of a topic model o...">Curran et al. (2018)</a> in the analysis of speeches in New Zealand discussed earlier. However, as our analysis covers a shorter time, we will not split the data into time slices. As seen in the literature, the analysis could be upgraded with structural topic modelling, where we could consider, for example, the party affiliation of the speakers and observe the differences among them. We could also analyse the entire ParlaMint-GB corpus and use dynamic topic modelling to observe the differences in time. Nevertheless, to compare the pre-pandemic and pandemic periods, the use of the LDA method is adequate.</p></div></div><div class="row"><div class="small-6 columns text-center"><p><a class="button" href="ch3.html" title="Previous: 3. Parliamentary debates">&lt;&lt;</a></p></div><div class="small-6 columns text-center"><p><a class="button" href="ch5.html" title="Next: 5. Preparing for the analysis">&gt;&gt;</a></p></div></div><!--Notes in [div]--><div class="notes"><div class="noteHeading">Notes</div><div class="note" id="ftn3"><p><a class="link_return" title="Pojdi nazaj k besedilu" href="#ftn3_return"><sup>3.</sup></a> <span class="noteBody">Time-based topic analysis can be done with plain LDA, but a separate topic model must be built for each period; the topics must be interpreted and then compared between time periods. Such an approach requires a lot more manual and subjective work, which can negatively affect the results. Another option for temporal topic analysis is <span style="font-style:italic" itemprop="hi">dynamic non-negative matrix factorisation</span> (<span style="font-style:italic" itemprop="hi">dynamic NMF</span>) (see <a class="link_ref" itemprop="ref" href="bibl.html#Gkoumas.2018" title="Gkoumas D. Pontiki M. Papanikolaou K.  Papageorgiou H. (2018). Exploring the Political Agenda of the Greek Parliament Plenary S...">Gkoumas et al. 2018</a>), which considers the time periods indirectly (<a class="link_ref" itemprop="ref" href="bibl.html#M%C3%BCller-Hansen.2021" title="MüllerHansen F. Callaghan M. W. Lee Y. T. Leipprand A. Flachsland C.  Minx J. C. (2021). Who cares about coal Analyzing 70 year...">Müller-Hansen et al. 2021</a>).</span></p></div></div><div class="row"><div class="small-6 columns text-center"><p><a class="button" href="ch3.html" title="Previous: 3. Parliamentary debates">&lt;&lt;</a></p></div><div class="small-6 columns text-center"><p><a class="button" href="ch5.html" title="Next: 5. Preparing for the analysis">&gt;&gt;</a></p></div></div></section></div><script type="text/javascript">
         
         $(function () {
         var viewer = ImageViewer();
         $('.imageviewer').click(function () {
         var imgSrc = this.src,
         highResolutionImage = $(this).data('high-res-src');
         viewer.show(imgSrc, highResolutionImage);
         });
         });
      </script><script src="https://www2.sistory.si/publikacije/themes/foundation/6/js/vendor/what-input.js"></script><script src="https://www2.sistory.si/publikacije/themes/foundation/6/js/vendor/foundation.min.js"></script><script src="https://www2.sistory.si/publikacije/themes/foundation/6/js/app.js"></script><script src="https://www2.sistory.si/publikacije/themes/js/plugin/back-to-top/back-to-top.js"></script></body></html>